{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7c3809",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xailib.data_loaders.dataframe_loader import prepare_dataframe\n",
    "\n",
    "from xailib.explainers.lime_explainer import LimeXAITabularExplainer\n",
    "from xailib.explainers.lore_explainer import LoreTabularExplainer\n",
    "from xailib.explainers.shap_explainer_tab import ShapXAITabularExplainer\n",
    "\n",
    "from xailib.models.sklearn_classifier_wrapper import sklearn_classifier_wrapper"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e2013bb7",
   "metadata": {},
   "source": [
    "# Learning and explaining German Credit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa104cfb",
   "metadata": {},
   "source": [
    "## Loading and preparation of data\n",
    "\n",
    "We start by reading from a CSV file the dataset to analyze. The table is loaded by means of the ```DataFrame``` class from the ```pandas``` library.\n",
    "\n",
    "Among all the attributes of the table, we select the ```class_field``` column that contains the observed class for the corresponding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679af295",
   "metadata": {},
   "source": [
    "source_file = 'datasets/german_credit.csv'\n",
    "class_field = 'default'\n",
    "# Load and transform dataset \n",
    "df = pd.read_csv(source_file, skipinitialspace=True, na_values='?', keep_default_na=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd0b372",
   "metadata": {},
   "source": [
    "After the data is loaded in memory, we need to extract metadata information to automatically handle the content withint the table.\n",
    "\n",
    "The method ```prepare_dataframe``` scans the table and extract the following information:\n",
    " * ```df```: is a trasformed version of the original dataframe, where discrete attributes are transformed into numerical attributes by using one hot encoding strategy;\n",
    " * ```feature_names```: is a list containint the names of the features after the transformation;\n",
    " * ```class_values```: the list of all the possible values for the ```class_field``` column;\n",
    " * ```numeric_columns```: a list of the original features that contain numeric (i.e. continuous) values;\n",
    " * ```rdf```: the original dataframe, before the transformation;\n",
    " * ```real_feature_names```: the list of the features of the dataframe before the transformation;\n",
    " * ```features_map```: it is a dictionary pointing each feature to the original one before the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96c2742",
   "metadata": {},
   "source": [
    "df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map = prepare_dataframe(df, class_field)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6cde0757",
   "metadata": {},
   "source": [
    "### Learning a Random Forest classfier\n",
    "\n",
    "We train a RF classifier by using the ```sklearn``` library. We start by splitting the dataset into a train and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa896e16",
   "metadata": {},
   "source": [
    "test_size = 0.3\n",
    "random_state = 42\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[feature_names], df[class_field],\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=df[class_field])\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a6bee110",
   "metadata": {},
   "source": [
    "Then we train the model on the training set. \n",
    "Once the model has been learned, we use a wrapper class to get access to the model for ```XAI lib```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4567b9",
   "metadata": {},
   "source": [
    "bb = RandomForestClassifier(n_estimators=20, random_state=random_state)\n",
    "bb.fit(X_train.values, Y_train.values)\n",
    "bbox = sklearn_classifier_wrapper(bb)   "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ce0c2f",
   "metadata": {},
   "source": [
    "Select a new instance to be classfied by the model and print the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1feaaed",
   "metadata": {},
   "source": [
    "inst = X_train.iloc[147].values\n",
    "print('Instance ',inst)\n",
    "print('True class ',Y_train.iloc[8])\n",
    "print('Predicted class ',bb.predict(inst.reshape(1, -1)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4b4bed0e",
   "metadata": {},
   "source": [
    "## Explaining the prediction\n",
    "We use the explanators of ```XAI lib``` to provide an explantion for the classified instance ```inst```.\n",
    "Every explainer of ```XAI lib``` takes in input the blackbox to be explained with the corresponding feature names, and a configuration object to initialize the explainer.\n",
    "\n",
    "### SHAP explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66282ada",
   "metadata": {},
   "source": [
    "explainer = ShapXAITabularExplainer(bbox, feature_names)\n",
    "config = {'explainer' : 'tree', 'X_train' : X_train.iloc[0:100].values}\n",
    "explainer.fit(config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dfaf6",
   "metadata": {},
   "source": [
    "exp = explainer.explain(inst)\n",
    "# print(exp.exp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdba8f",
   "metadata": {},
   "source": [
    "exp.plot_features_importance()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "41b9a6e2",
   "metadata": {},
   "source": [
    "### LORE explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca2418",
   "metadata": {},
   "source": [
    "explainer = LoreTabularExplainer(bbox)\n",
    "config = {'neigh_type':'rndgen', 'size':1000, 'ocr':0.1, 'ngen':10}\n",
    "explainer.fit(df, class_field, config)\n",
    "exp = explainer.explain(inst)\n",
    "print(exp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecf3c0",
   "metadata": {},
   "source": [
    "exp.plotRules()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae00673",
   "metadata": {},
   "source": [
    "exp.plotCounterfactualRules()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ad54cf36",
   "metadata": {},
   "source": [
    "### LIME explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4507a11",
   "metadata": {},
   "source": [
    "limeExplainer = LimeXAITabularExplainer(bbox)\n",
    "config = {'feature_selection': 'lasso_path'}\n",
    "limeExplainer.fit(df, class_field, config)\n",
    "lime_exp = limeExplainer.explain(inst)\n",
    "print(lime_exp.exp.as_list())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bda293c",
   "metadata": {},
   "source": [
    "# limeExplainer.plot_lime_values(lime_exp.as_list(), 5, 10)\n",
    "lime_exp.plot_features_importance()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ba2a36",
   "metadata": {},
   "source": [
    "## Learning a different model\n",
    "\n",
    "### Learning a Logistic Regressor\n",
    "\n",
    "We train a Logistic Regression by using the ```sklearn``` library. We transform the dataset by using a ```Scaler``` to normalize all the attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136908cb",
   "metadata": {},
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "bb = LogisticRegression(C=1, penalty='l2')\n",
    "bb.fit(X_scaled, Y_train.values)\n",
    "# pass the model to the wrapper to use it in the XAI lib\n",
    "bbox = sklearn_classifier_wrapper(bb)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e9178d",
   "metadata": {},
   "source": [
    "# select a record to explain\n",
    "inst = X_scaled[182]\n",
    "print('Instance ',inst)\n",
    "print('Predicted class ',bb.predict(inst.reshape(1, -1)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2064387e",
   "metadata": {},
   "source": [
    "## Explaining the prediction\n",
    "We use the same explainators as for the previous model. In this case, a few adjustments are necessary for the initialization of the explanators. For example, SHAP needs a specific configuration for the linear model we are using.\n",
    "### SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8174c86",
   "metadata": {},
   "source": [
    "explainer = ShapXAITabularExplainer(bbox, feature_names)\n",
    "config = {'explainer' : 'linear', 'X_train' : X_scaled[0:100], 'feature_pert' : 'interventional'}\n",
    "explainer.fit(config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c23ef04",
   "metadata": {},
   "source": [
    "exp = explainer.explain(inst)\n",
    "print(exp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6172bf52",
   "metadata": {},
   "source": [
    "exp.plot_features_importance()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "99eebd04",
   "metadata": {},
   "source": [
    "### LORE explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32efedbe",
   "metadata": {},
   "source": [
    "explainer = LoreTabularExplainer(bbox)\n",
    "config = {'neigh_type':'geneticp', 'size':1000, 'ocr':0.1, 'ngen':10}\n",
    "explainer.fit(df, class_field, config)\n",
    "exp = explainer.explain(inst)\n",
    "print(exp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e0c8b6",
   "metadata": {},
   "source": [
    "exp.plotRules()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f96dc1",
   "metadata": {},
   "source": [
    "exp.plotCounterfactualRules()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a84cc811",
   "metadata": {},
   "source": [
    "### LIME explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3485f425",
   "metadata": {},
   "source": [
    "limeExplainer = LimeXAITabularExplainer(bbox)\n",
    "config = {'feature_selection': 'lasso_path'}\n",
    "limeExplainer.fit(df, class_field, config)\n",
    "lime_exp = limeExplainer.explain(inst)\n",
    "print(lime_exp.exp.as_list())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dae4819",
   "metadata": {},
   "source": [
    "lime_exp.plot_features_importance()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fde98",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
